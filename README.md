# YoutubeResumer
![OIG1](https://github.com/user-attachments/assets/c4eab0b7-4504-4782-991b-81a2601fd0f7)


VocÃª jÃ¡ quis resumir vÃ­deos longos do YouTube em poucos minutos, extraindo apenas as informaÃ§Ãµes mais importantes para saber se vale Ã  pena assistir? 

Pra isso eu pensei no "Resumidor de VÃ­deos do Youtube". Mas como queria um nome mais chique e criativoğŸ™„, ficou YoutubeResumer =)

Neste tutorial vamos criar  um resumidor de vÃ­deos do YouTube usando **Python** e **inteligÃªncia artificial** local com o ollama. 

Objetivos:

- Descobrir como extrair a transcriÃ§Ã£o de um vÃ­deo,
- processar a linguagem natural com tÃ©cnicas de IA e
- gerar resumos concisos,

tudo isso de maneira automatizada e rodando suave.


-----------------------


# 1. Instalando os programas


Os requisitos para que o programa funcione sÃ£o:

- InstalaÃ§Ã£o do Python e das bibliotecas (uma para transcriÃ§Ã£o do vÃ­deo e uma para a IA)
- InstalaÃ§Ã£o do Ollama


### 1.1 - Instalando o ollama


Caso ainda nÃ£o tenha o ollama instalado, faÃ§a-o agora mesmo! Quem sabe algum dia sem internet vocÃª quer conversar com uma IA, ou no caso de algum evento imprevisto (apocalipse zumbi), ou para manter sua privacidade, ou para realizar testes... Eu realmente recomendo ter o ollama, Ã© simples de instalar e muito poderoso. 

- Para rodar o Ollama, vÃ¡ atÃ© o site www.ollama.com, escolha sua plataforma e instale o programa.
- Em seguida, abra o shell (terminal) ou prompt de comando e digite:

	ollama run llama3.2

caso tudo tenha dado certo, vocÃª jÃ¡ estÃ¡ o modelo instalado localmente. 

![Pasted image 20241014162130](https://github.com/user-attachments/assets/905a5842-10e2-4cf6-ba94-75657329f991)


Caso deseje obter um resumo por mais de um modelo, instale quais desejar.
Dependendo da capacidade de seu hardware, especialmente da placa de vÃ­deo (GPU), poderÃ¡ instalar modelos maiores. 

Se sua mÃ¡quina tiver que usar a CPU ao invÃ©s da GPU o tempo de processamento serÃ¡ maior, mas alguns modelos tem uma eficiÃªncia tÃ£o boa que permitem um tempo de resposta relativamente baixo. 

Para computadores comuns, o melhor custo-benefÃ­cio na minha (modesta e imperfeita) opiniÃ£o, estÃ¡ em:


[**Phi 3.5**](https://ollama.com/library/phi3.5)

O  modelo da Microsoft, com 3.8 bilhÃµes de parÃ¢metros, deve rodar bem mesmo em desktops comuns. Para instalÃ¡-lo digite:

	ollama pull phi3.5


[**Llama 3.2**](https://ollama.com/library/llama3.2)

Com uma capacidade tambÃ©m incrÃ­vel com seus 3.21 bilhÃµes de parÃ¢metros, o llama 3.2 da Meta Ã© uma opÃ§Ã£o funcional e leve para mÃ¡quinas desktop. TambÃ©m disponÃ­vel na versÃ£o de 1 bilhÃ£o de parÃ¢metros, para maior velocidade e alta capacidade em dispositivos simples. 

	ollama pull llama3.2               # para o modelo de 3 bilhÃµes de parÃ¢metros
	ollama pull llama3.2:1b            # para o modelo de 1 bilhÃ£o de parÃ¢metros
	ollama pull llama3.2:27b           # para o modelo de 27 bilhÃµes de parÃ¢metros


[Llama 3.1]([llama3.1 (ollama.com)](https://ollama.com/library/llama3.1))

TambÃ©m da Meta, o Llama 3.1 Ã© um pouco mais pesado que seu sucessor, mas extremamente poderoso. Com seus 8 bilhÃµes de parÃ¢metros, Ã© atÃ© o momento um dos melhores modelos abertos disponÃ­veis. 

	ollama pull llama3.1              # para o modelo com 8 bilhÃµes de parÃ¢metros
	ollama pull llama3.1:70b          # para o modelo com 70 bilhÃµes de parÃ¢metros
	ollama pull llama3.1:405b         # para o modelo com 405 bilhÃµes de parÃ¢metros


[Gemma2](https://ollama.com/library/gemma2)

Diretamente dos laboratÃ³rios do Google, o Gemma 2 Ã© um modelo de alta performance e disponÃ­vel em 2b, 7b e 29b.

	ollama pull gemma2              # para o modelo com 7 bilhÃµes de parÃ¢metros
	ollama pull gemma2:2b           # para o modelo com 2 bilhÃµes de parÃ¢metros
	ollama pull gemma:27b           # para o modelo com 27 bilhÃµes de parÃ¢metros


[Mistral-Nemo](https://ollama.com/library/mistral-nemo)

O modelo de 12b desenvolvido pela Nvidia, com 128k de contexto (pode lidar com uma grande quantidade de tokens na entrada). 

	ollama pull mistral-nemo



***Caso queira saber quanto um modelo estÃ¡ usando de memÃ³ria, ou se ele estÃ¡ totalmente carregado em sua GPU ou CPU, ou se estÃ¡ em ambas, utilize o comando***

	ollama ps

![image](https://github.com/user-attachments/assets/a0b01cc6-c71c-473f-ada9-d8163b369efc)



### 1.2 - Python


**Windows:** 

- vocÃª pode escolher a versÃ£o diretamente da Microsoft Store, ou em [Download Python | Python.org](https://www.python.org/downloads/)

**Linux:** 

- utilize o comando de instalaÃ§Ã£o para sua versÃ£o, por exemplo no Debian:

	sudo apt install python3.11-full python3-pip


![Pasted image 20241014163228](https://github.com/user-attachments/assets/80cfe662-f0a0-41e2-8c2f-932bbb67b7bf)



### 1.3 - Instalando as dependÃªncias necessÃ¡rias



Utilize o comando abaixo para instalar as dependÃªncias necessÃ¡rias desse projeto, a api para transcrever o vÃ­deo e a que conectarÃ¡ ao ollama: 

	pip install  youtube_transcript_api ollama


------------------------------------------

# 2. Funcionalidades


### 2.1 Obtendo a transcriÃ§Ã£o


O programa [**get_transcript.py**](https://github.com/gazstao/YoutubeResumer/blob/main/get_transcript.py) faz a transcriÃ§Ã£o de um vÃ­deo. 


	#biblioteca para lidar com a API da Youtube
	from youtube_transcript_api import YouTubeTranscriptApi Â  Â  Â  Â  Â  Â  Â  Â 
	idioma = ['pt','en']
	
	# Obtem a URL do vÃ­deo
	video_url = input ("Qual a url do video que deseja transcrever?\n") Â  Â  
	
	# Obtem o id a partir da URL
	video_id = video_url.split("v=")[-1] Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
	
	try:
		transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=idioma) Â  Â 
		transcript_text = " ".join([entry['text'] for entry in transcript]) Â  Â  Â  Â  Â  Â 
		print(transcript_text)
	
	except Exception as e:
		print(f"Erro ao obter a transcriÃ§Ã£o na funÃ§Ã£o get_transcript: {e}")



### 2.2 Solicitando o resumo

