# YoutubeResumer

![OIG1](https://github.com/user-attachments/assets/c4eab0b7-4504-4782-991b-81a2601fd0f7)


Voc√™ j√° quis resumir v√≠deos longos do YouTube em poucos minutos, extraindo apenas as informa√ß√µes mais importantes para saber se vale √† pena assistir? 

Pra isso eu pensei no "Resumidor de V√≠deos do Youtube". Mas como queria um nome mais chique e criativoüôÑ, ficou YoutubeResumer =)

Neste tutorial vamos criar  um resumidor de v√≠deos do YouTube usando **Python** e **intelig√™ncia artificial** local com o ollama. 

Objetivos:

- Descobrir como extrair a transcri√ß√£o de um v√≠deo,
- processar a linguagem natural com t√©cnicas de IA e
- gerar resumos concisos,

tudo isso de maneira automatizada e rodando suave.


# 1. Instalando os programas


Os requisitos para que o programa funcione s√£o:

- Instala√ß√£o do Python e das bibliotecas (uma para transcri√ß√£o do v√≠deo e uma para o ollama)
- Instala√ß√£o do Ollama


## 1.1 - Instalando o ollama


Caso ainda n√£o tenha o ollama instalado, fa√ßa-o agora mesmo! Quem sabe algum dia sem internet voc√™ quer conversar com uma IA, ou no caso de algum evento imprevisto (apocalipse zumbi), ou para manter sua privacidade, ou para realizar testes... Eu realmente recomendo ter o ollama, √© simples de instalar e muito poderoso. 

- Para rodar o Ollama, v√° at√© o site www.ollama.com, escolha sua plataforma e instale o programa.
- Em seguida, abra o shell (terminal) ou prompt de comando e digite:

	ollama run llama3.2

caso tudo tenha dado certo, voc√™ j√° est√° o modelo instalado localmente. 

![Pasted image 20241014162130](https://github.com/user-attachments/assets/905a5842-10e2-4cf6-ba94-75657329f991)


Caso deseje obter um resumo por mais de um modelo, instale quais desejar.
Dependendo da capacidade de seu hardware, especialmente da placa de v√≠deo (GPU), poder√° instalar modelos maiores. 

Se sua m√°quina tiver que usar a CPU ao inv√©s da GPU o tempo de processamento ser√° maior, mas alguns modelos tem uma efici√™ncia t√£o boa que permitem um tempo de resposta relativamente baixo. 

Para computadores comuns, o melhor custo-benef√≠cio na minha (modesta e imperfeita) opini√£o, est√° em:


[**Phi 3.5**](https://ollama.com/library/phi3.5)

O  modelo da Microsoft, com 3.8 bilh√µes de par√¢metros, deve rodar bem mesmo em desktops comuns. Para instal√°-lo digite:

	ollama pull phi3.5


[**Llama 3.2**](https://ollama.com/library/llama3.2)

Com uma capacidade tamb√©m incr√≠vel com seus 3.21 bilh√µes de par√¢metros, o llama 3.2 da Meta √© uma op√ß√£o funcional e leve para m√°quinas desktop. Tamb√©m dispon√≠vel na vers√£o de 1 bilh√£o de par√¢metros, para maior velocidade e alta capacidade em dispositivos simples. 

	ollama pull llama3.2               # para o modelo de 3 bilh√µes de par√¢metros
	ollama pull llama3.2:1b            # para o modelo de 1 bilh√£o de par√¢metros
	ollama pull llama3.2:27b           # para o modelo de 27 bilh√µes de par√¢metros


[Llama 3.1]([llama3.1 (ollama.com)](https://ollama.com/library/llama3.1))

Tamb√©m da Meta, o Llama 3.1 √© um pouco mais pesado que seu sucessor, mas extremamente poderoso. Com seus 8 bilh√µes de par√¢metros, √© at√© o momento um dos melhores modelos abertos dispon√≠veis. 

	ollama pull llama3.1              # para o modelo com 8 bilh√µes de par√¢metros
	ollama pull llama3.1:70b          # para o modelo com 70 bilh√µes de par√¢metros
	ollama pull llama3.1:405b         # para o modelo com 405 bilh√µes de par√¢metros


[Gemma2](https://ollama.com/library/gemma2)

Diretamente dos laborat√≥rios do Google, o Gemma 2 √© um modelo de alta performance e dispon√≠vel em 2b, 7b e 29b.

	ollama pull gemma2              # para o modelo com 7 bilh√µes de par√¢metros
	ollama pull gemma2:2b           # para o modelo com 2 bilh√µes de par√¢metros
	ollama pull gemma:27b           # para o modelo com 27 bilh√µes de par√¢metros


[Mistral-Nemo](https://ollama.com/library/mistral-nemo)

O modelo de 12b desenvolvido pela Nvidia, com 128k de contexto (pode lidar com uma grande quantidade de tokens na entrada). 

	ollama pull mistral-nemo



***Caso queira saber quanto um modelo est√° usando de mem√≥ria, ou se ele est√° totalmente carregado em sua GPU ou CPU, ou se est√° em ambas, utilize o comando***

	ollama ps

![image](https://github.com/user-attachments/assets/a0b01cc6-c71c-473f-ada9-d8163b369efc)



## 1.2 - Python


**Windows:** 

- voc√™ pode escolher a vers√£o diretamente da Microsoft Store, ou em [Download Python | Python.org](https://www.python.org/downloads/)

**Linux:** 

- utilize o comando de instala√ß√£o para sua vers√£o, por exemplo no Debian:

	sudo apt install python3.11-full python3-pip


![Pasted image 20241014163228](https://github.com/user-attachments/assets/80cfe662-f0a0-41e2-8c2f-932bbb67b7bf)



## 1.3 - Instalando as depend√™ncias necess√°rias



Utilize o comando abaixo para instalar as depend√™ncias necess√°rias desse projeto, a api para transcrever o v√≠deo e a que conectar√° ao ollama: 

	pip install  youtube_transcript_api ollama



# 2. Funcionalidades


## 2.1 Obtendo a transcri√ß√£o


O programa [**get_transcript.py**](https://github.com/gazstao/YoutubeResumer/blob/main/get_transcript.py) faz a transcri√ß√£o de um v√≠deo. 

	python3 get_transcript.py


## 2.2 Solicitando o resumo

O programa [**resuma.py**](https://github.com/gazstao/YoutubeResumer/blob/main/resuma.py)faz a transcri√ß√£o seguida de um resumo do v√≠deo.

	python3 resuma.py



# 3. Vers√£o Final

A vers√£o final [**youtubeResumer.py**](https://github.com/gazstao/YoutubeResumer/blob/main/youtubeResumer.py) possui algumas funcionalidades adicionais, que s√£o gravar a transcri√ß√£o em um arquivo de texto e criar um arquivo html com cada resumo. Para gravas esses arquivos, deve existir uma pasta chamada "data" em seu diret√≥rio.

-  Ir√° obter a transcri√ß√£o e grav√°-la em um arquivo de texto na pasta ./data
-  Ir√° criar um arquivo html para cada resposta com o resumo na pasta ./data

![image](https://github.com/user-attachments/assets/c840298a-a41c-435e-8283-d1dcd9d75e2a)


